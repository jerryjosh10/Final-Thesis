{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcptEmOSsNvY"
      },
      "outputs": [],
      "source": [
        "!pip install -q keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_u-6tu7p16Z",
        "outputId": "76cc0c7d-0a0b-45ea-d599-49a0895993e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 2/20 [==>...........................] - ETA: 1:12:29 - loss: 2.4667 - accuracy: 0.1758"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import plot_model\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Load the dataset\n",
        "train_file_path = '/content/train.tsv'\n",
        "valid_file_path = '/content/valid.tsv'\n",
        "test_file_path = '/content/test.tsv'\n",
        "\n",
        "train_data = pd.read_csv(train_file_path, sep='\\t', encoding=\"Latin-1\")\n",
        "valid_data = pd.read_csv(valid_file_path, sep='\\t', encoding=\"Latin-1\")\n",
        "test_data = pd.read_csv(test_file_path, sep='\\t', encoding=\"Latin-1\")\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|@[^\\s]+|#\\w+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s\\d]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "train_data['Text'] = train_data['Text'].apply(preprocess_text)\n",
        "valid_data['Text'] = valid_data['Text'].apply(preprocess_text)\n",
        "test_data['Text'] = test_data['Text'].apply(preprocess_text)\n",
        "\n",
        "# Tokenize text using BERT\n",
        "maxlen = 100\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = tokenizer.encode_plus(text, max_length=maxlen, truncation=True, padding='max_length', return_tensors='tf')\n",
        "    return tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "# Tokenize train, validation, and test data\n",
        "train_tokens = [tokenize_text(text) for text in train_data['Text']]\n",
        "val_tokens = [tokenize_text(text) for text in valid_data['Text']]\n",
        "test_tokens = [tokenize_text(text) for text in test_data['Text']]\n",
        "\n",
        "# Stack input_ids and attention_masks\n",
        "X_train_input_ids = np.vstack([token[0] for token in train_tokens])\n",
        "X_train_attention_masks = np.vstack([token[1] for token in train_tokens])\n",
        "\n",
        "X_val_input_ids = np.vstack([token[0] for token in val_tokens])\n",
        "X_val_attention_masks = np.vstack([token[1] for token in val_tokens])\n",
        "\n",
        "X_test_input_ids = np.vstack([token[0] for token in test_tokens])\n",
        "X_test_attention_masks = np.vstack([token[1] for token in test_tokens])\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_data['Label'])\n",
        "valid_labels = label_encoder.transform(valid_data['Label'])\n",
        "test_labels = label_encoder.transform(test_data['Labels'])\n",
        "\n",
        "# Define the BERT model\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Freeze BERT layers\n",
        "bert_model.trainable = False\n",
        "\n",
        "# Define the CNN model\n",
        "bert_input_ids = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "bert_attention_masks = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "\n",
        "# Get BERT embeddings\n",
        "embedding = bert_model(bert_input_ids, attention_mask=bert_attention_masks)[0]  # Use token embeddings\n",
        "\n",
        "# CNN layer\n",
        "conv_layer = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu')(embedding)\n",
        "maxpool_layer = tf.keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
        "\n",
        "# Output layer\n",
        "output = tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')(maxpool_layer)\n",
        "\n",
        "# Build model\n",
        "model = tf.keras.models.Model(inputs=[bert_input_ids, bert_attention_masks], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit([X_train_input_ids, X_train_attention_masks], train_labels,\n",
        "                    epochs=10, batch_size=512,\n",
        "                    validation_data=([X_val_input_ids, X_val_attention_masks], valid_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate([X_test_input_ids, X_test_attention_masks], test_labels)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "\n",
        "val_loss, val_accuracy = model.evaluate([X_val_input_ids, X_val_attention_masks], valid_labels)\n",
        "print(\"Valid Loss:\", val_loss)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fD_8p4Tmm50"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# true labels and predicted probabilities from your dataset\n",
        "true_labels = X_val_attention_masks\n",
        "predicted_probabilities = model.predict(X_val_input_ids)\n",
        "\n",
        "# Threshold the predicted probabilities to obtain binary predictions\n",
        "predicted_labels = (predicted_probabilities > 0.5).astype(int)\n",
        "\n",
        "# Flatten the predicted labels array\n",
        "predicted_labels_flattened = predicted_labels.flatten()\n",
        "\n",
        "# Reshape the predicted labels array to match the shape of the true labels array\n",
        "predicted_labels_reshaped = predicted_labels_flattened[:len(true_labels)]\n",
        "\n",
        "# Compute true positives, false positives, false negatives\n",
        "tp = np.sum((true_labels == 1) & (predicted_labels_reshaped == 1))\n",
        "fp = np.sum((true_labels == 0) & (predicted_labels_reshaped == 1))\n",
        "fn = np.sum((true_labels == 1) & (predicted_labels_reshaped == 0))\n",
        "\n",
        "# Compute precision, recall, and F1 score\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
